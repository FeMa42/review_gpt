{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review generator for papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.pdf_to_text import pdf_to_sections\n",
    "from utils.summary_creator_gpt import create_summary_chatgpt\n",
    "from utils.large_text_handler import split_text_file_by_keywords\n",
    "from utils.tex_to_text import tex_to_sections\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "You can set the input file. You can us a tex file or a pdf file. If you use a pdf file you can also set the keywords for the sections (make sure that they are unique). The tex file gets splitted based on the section command. \n",
    "\n",
    "If is_summary is False ChatGPT will be used to generate a review. If is_summary is True ChatGPT will be used to generate a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file = './texts/SOIL/soiltdm.tex'\n",
    "# projekt_name = 'SOIL_TDM'\n",
    "\n",
    "input_file = '/Users/damian/soiltdm.pdf'\n",
    "projekt_name = 'SOIL_TDM_PDF'\n",
    "\n",
    "# Extract text from PDF\n",
    "extract_method = 'pdf'  # 'pdf', 'tex', 'txt-whole', 'txt-split'\n",
    "\n",
    "model = 'gpt-3.5-turbo'  # use chatgpt for review\n",
    "is_summary = False  # if false we review the text, if true we create a summary\n",
    "\n",
    "# Keywords to split the text into sections for pdf review, summary\n",
    "keywords = ['Abstract', '1 Introduction', '2 Background',\n",
    "            '3 Method', '4 Related Work', '5 Experiments', '6 Conclusion']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working directory\n",
    "working_dir = os.path.join(\"./texts/\", projekt_name)\n",
    "if not os.path.isdir(working_dir):\n",
    "    os.mkdir(working_dir)\n",
    "\n",
    "section_titles = []\n",
    "extract_path = os.path.join(working_dir, 'extract.txt')\n",
    "if extract_method == 'pdf':\n",
    "    # Extract text from PDF\n",
    "    print(\"Extracting text from PDF file\")\n",
    "    # Split large text into chunks based on keywords\n",
    "    sections = pdf_to_sections(input_file, working_dir, keywords)\n",
    "    print(\"Extracted text from PDF file\")\n",
    "elif extract_method == 'tex':\n",
    "    # Extract text from tex file.\n",
    "    # This can work better than the pdf extraction, because there might be less formatting issues.\n",
    "    print(\"Extracting text from tex file\")\n",
    "    sections = tex_to_sections(\n",
    "        input_file, working_dir, extract_file='extract.txt')\n",
    "    print(\"Extracted text from tex file\")\n",
    "elif extract_method == 'txt-split':\n",
    "    # Work with already split text files.\n",
    "    # This can be helpfull if you want to finetune the chunks by hand or if you want to use a different splitting method.\n",
    "    section_files = []\n",
    "    i = 1\n",
    "    while True:\n",
    "        try:\n",
    "            filename = f'section_{i+1}.txt'\n",
    "            file_path = os.path.join(working_dir, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as input_file:\n",
    "                content = input_file.read()\n",
    "                section_files.append(content)\n",
    "                i += 1\n",
    "        except FileNotFoundError:\n",
    "            break\n",
    "    # Now section_files is a list containing the content of each section file\n",
    "    sections = []\n",
    "    print(\"found \" + str(len(section_files)) + \" section files\")\n",
    "    if len(section_files) <= len(section_titles):\n",
    "        print(\"found less than or equal amount of section files compared to keywords\")\n",
    "        use_section_titles = True\n",
    "    else:\n",
    "        print(\"found more section files than section titles\")\n",
    "        use_section_titles = False\n",
    "    for idx, content in enumerate(section_files):\n",
    "        if use_section_titles:\n",
    "            title = section_titles[idx]\n",
    "        else:\n",
    "            title = f'section_{idx+1}.txt'\n",
    "        content = content\n",
    "        sections.append({\n",
    "            'title': title,\n",
    "            'content': content\n",
    "        })\n",
    "else:  # extract_method == 'txt-whole':\n",
    "    # use whole text file, and split it into chunks based on keywords. If you modified the text file by hand, you can use this method.\n",
    "    # Please note that linebreaks are removed before the text is split into chunks.\n",
    "    print(\"Reading text from file\")\n",
    "    with open(extract_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    # Split large text into chunks based on keywords\n",
    "    sections = split_text_file_by_keywords(\n",
    "        text, working_dir, keywords,\n",
    "        clean_extracted_chunk=True,\n",
    "        add_spaces_to_chunk=False)  # add_spaces_to_chunk adds spaces based on word frequency. Activate if there are errors in the text\n",
    "    print(\"Extracted text from text file\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run reviewer based on ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_summary:\n",
    "    method = 'summary'\n",
    "else:\n",
    "    method = 'review'\n",
    "combined_summary_path = os.path.join(\n",
    "    working_dir, f'{method} using {model} .txt')\n",
    "with open(combined_summary_path, 'w', encoding='utf-8') as output_file:\n",
    "    for section in sections:\n",
    "        content = section['content']\n",
    "        title = section['title']\n",
    "        print(f\"Start {method} of {title} using {model} ...\")\n",
    "        summary_part, finish_reason = create_summary_chatgpt(\n",
    "            content, section=f\"{title} section\", model=model, is_summary=False)\n",
    "        print(\n",
    "            f\"Finished {method} of {title} using {model} because {finish_reason}\")\n",
    "        output_file.write(f' {method} of {title}\\n')\n",
    "        output_file.write(summary_part)\n",
    "        output_file.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
